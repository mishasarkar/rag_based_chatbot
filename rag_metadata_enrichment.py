###
# pip install pymupdf
# pip install spacy
# python -m spacy download en_core_web_sm
###


from tqdm.rich import trange, tqdm
from rich import console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.text import Text
import warnings
warnings.filterwarnings(action='ignore')
import datetime
from rich.console import Console
console = Console(width=110)
import os
import spacy
import fitz
from langchain.document_loaders import TextLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.schema.document import Document
from collections import Counter

# Load spaCy model
nlp = spacy.load('en_core_web_sm')


logfile = 'KeyBERT-Log.txt'


def writehistory(text):
   with open(logfile, 'a', encoding='utf-8') as f:
       f.write(text)
       f.write('\n')
   f.close()


def extract_keys_spacy(text):
   doc = nlp(text)
   # Extracting noun chunks and named entities as keywords
   keywords = set()
   for token in doc:
       if token.is_stop or token.is_punct:
           continue
       if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB']:
           keywords.add(token.lemma_.lower())           
   # for chunk in doc.noun_chunks:
   #     keywords.add(chunk.text.lower())
   # for ent in doc.ents:
   #     keywords.add(ent.text.lower())
   return list(keywords)


def extract_keys_spacy2(text):
   # Remove stop words and punctuation symbols
   words = [token.text for token in text if not token.is_stop and not token.is_punct]
   word_freq = Counter(words)
   # 5 commonly occurring words with their frequencies
   common_words = word_freq.most_common(5)
   print(common_words)
   # Unique words
   unique_words = [word for (word, freq) in word_freq.items() if freq == 1]
   print(unique_words)   




text = """
Title: Magicoder: Source Code Is All You Need
published on PaperswithCode
Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang
url: https://arxiv.org/pdf/2312.02120v1.pdf
We introduce Magicoder, a series of fully open-source (code, weights, and data)
Large Language Models (LLMs) for code that significantly closes the gap with top
code models while having no more than 7B parameters. Magicoder models are
trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach
to enlightening LLMs with open-source code snippets to generate high-quality
instruction data for code. Our main motivation is to mitigate the inherent bias of
the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable
data. The orthogonality of OSS-INSTRUCT and other data generation methods
like Evol-Instruct further enables us to build an enhanced MagicoderS. Both
Magicoder and MagicoderS substantially outperform state-of-the-art code models
with similar or even larger sizes on a wide range of coding benchmarks, including
Python text-to-code generation, multilingual coding, and data-science program
completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses
the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall,
OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction
tuning using abundant open-source references.
"""






keywords = extract_keys_spacy(text)
console.print(f"[bold]Keywords: {keywords}")


extract_keys_spacy2(nlp(text))


filename = 'Magicoder.pdf'
# Read the above pdf file and extract text in a variable using PyMuPDF


pdf_document = fitz.open(filename)
fulltext = ""
for page_num in range(len(pdf_document)):
   page = pdf_document[page_num]
   fulltext += page.get_text()


keywords = extract_keys_spacy(fulltext)
console.print(f"\n\n [bold]Keywords: {keywords}")


title = 'Magicoder Source Code Is All You Need'
filename = 'Magicoder Source Code Is All You Need.txt'
author = 'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'
url = 'https://arxiv.org/pdf/2312.02120v1.pdf'




text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=10)
splitted_text = text_splitter.split_text(fulltext)


console.print("\n\n --- Printing texts of pdf ---")
console.print(len(splitted_text))
console.print("---")
console.print(splitted_text[0])


keys = []
for i in trange(0, len(splitted_text)):
   text = splitted_text[i]
   keys.append({'document': filename,
                'title': title,
                'author': author,
                'url': url,
                'doc': text,
                'keywords': extract_keys_spacy(text)
                })
console.print("\n\n --- Printing keys ---")
console.print(keys[1])


goodDocs = []
for i in range(0, len(keys)):
   goodDocs.append(Document(page_content=keys[i]['doc'],
                             metadata={'source': keys[i]['document'],
                                       'type': 'chunk',
                                       'title': keys[i]['title'],
                                       'author': keys[i]['author'],
                                       'url': keys[i]['url'],
                                       'keywords': keys[i]['keywords']
                                       }))
console.print("\n\n --- Printing Docs ---")
console.print(goodDocs[1])